{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### python notebook for Make Your Own Neural Network\n",
    "based on Tariq Rashid, original 2016, \n",
    "license is GPLv2\n",
    "\n",
    "The original book uses these datasets:\n",
    "- A training set http://www.pjreddie.com/media/files/mnist_train.csv\n",
    "- A test set http://www.pjreddie.com/media/files/mnist_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# scipy.special for the sigmoid function expit()\n",
    "import scipy.special\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the mnist training data CSV file into a list\n",
    "training_data_file = open(\"data/MNIST/small/mnist_train_100.csv\", 'r')\n",
    "#training_data_file = gzip.open(\"data/MNIST/mnist_train.csv.gz\", 'rt')\n",
    "# always gzip your data\n",
    "\n",
    "training_data_list = training_data_file.readlines()[1:]\n",
    "training_data_file.close()\n",
    "len(training_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,42,154,180,255,176,118,118,16,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,142,253,253,253,253,253,253,236,103,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,227,253,253,204,177,177,177,243,191,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,227,253,216,22,0,0,23,227,238,96,21,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,227,253,205,0,0,17,124,253,253,253,170,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,227,253,234,62,18,201,253,253,253,251,90,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,227,253,253,253,253,253,253,253,221,103,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,227,253,253,253,253,253,208,24,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,39,236,253,253,253,251,97,16,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,69,224,253,253,240,169,46,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,33,134,253,253,253,253,105,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,51,225,253,253,253,253,253,68,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,48,227,253,253,250,174,253,253,68,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,213,253,253,179,63,111,253,253,68,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,92,251,201,13,5,0,166,253,253,68,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,39,222,253,198,0,0,0,248,253,231,46,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,54,243,253,124,0,38,133,252,253,150,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,118,253,253,237,179,223,253,253,190,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,44,230,253,253,253,253,253,244,76,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,231,253,253,253,182,66,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(training_data_list[30]))\n",
    "training_data_list[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number is the label, and the rest of the 784 numbers are the colour values for the pixels that make up the image. These colour values range between 0 and 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 0: \n",
    "Plot this rectangular array of numbers using the *imshow()* function. We need to convert that list of comma separated numbers into a suitable array. Here are the steps to do that:\n",
    "- Split that long text string of comma separated values into individual values, using the commas as the place to do the splitting.\n",
    "- Ignore the first value, which is the label, and take the remaining list of 28 * 28 = 784 values and turn them into an array which has a shape of 28 rows by 28 columns.\n",
    "- Plot that array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/BYONN_Task0.py\n",
    "#Task 0: Plot this rectangular array of numbers using the imshow() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mnist test data CSV file into a list\n",
    "test_data_file = open(\"data/MNIST/small/mnist_test_10.csv\", 'r')\n",
    "#test_data_file = gzip.open(\"data/MNIST/small/mnist_test_10.csv.gz\", 'rt')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1: plot the sigmoid function expit()\n",
    "\n",
    "#%load solutions/BYONN_Task1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1a: What is the first derivative of the sigmoid function?\n",
    "# Plot this function\n",
    "\n",
    "#%load solutions/BYONN_Task1a.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/BYONN_Task2.py\n",
    "#Task 2: initialize one random (normally distributed) matrix pxn, \n",
    "#       where n=784 and p = 100 \n",
    "# wih = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3: multiply this matrix with the first MNIST digit vector \n",
    "#        (which you should rescale first to lie between 0.01 and 1)\n",
    "\n",
    "# %load solutions/BYONN_Task3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  4.,  3.,  2.,  1.],\n",
       "       [10.,  8.,  6.,  4.,  2.],\n",
       "       [15., 12.,  9.,  6.,  3.],\n",
       "       [20., 16., 12.,  8.,  4.],\n",
       "       [25., 20., 15., 10.,  5.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load solutions/BYONN_Task4.py\n",
    "#Task 4: unusual matrix multiplication: \n",
    "# 1. create a column vector x which is simply the sequence 1:5\n",
    "# 2. compute the dot product of x with its transpose \n",
    "\n",
    "x = numpy.linspace(1, 5,num=5)\n",
    "y = numpy.linspace(5, 1,num=5)\n",
    "#for kicks: inner product\n",
    "numpy.dot(x.reshape(1,5),x.reshape(5,1))\n",
    "#\"outer product\"\n",
    "numpy.dot(x.reshape(5,1),y.reshape(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 5: apply the sigmoid function to the resulting vector from task 3\n",
    "#        \n",
    "# %load solutions/BYONN_Task5.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 5a: create a one hot encoding vector for e.g. the label 2\n",
    "#         instead of (0,1), use (0.01,0.99) encoding\n",
    "\n",
    "# %load solutions/BYONN_Task5a.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/BYONN_Task6.py\n",
    "#Task 6: Complete the code to compute a forward pass through a NN\n",
    "\n",
    "\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60489673],\n",
       "       [0.56831344],\n",
       "       [0.49545433]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load solutions/BYONN_Task7.py\n",
    "#Task 7: run a simple query:\n",
    "\n",
    "#number of input, hidden and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "\n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)\n",
    "\n",
    "# test query (doesn't mean anything useful yet)\n",
    "input=[1.0, 0.5, -1.5]\n",
    "n.query(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/BYONN_Task8.py\n",
    "#Task 8: train the neural network:\n",
    "\n",
    "# %load BYONN_Task8.py\n",
    "\n",
    "# train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # convert targets list to 2d array\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/BYONN_Task8a.py\n",
    "## Redefine the complete neural network class (copy paste):\n",
    "#%load BYONN_Task8a.py\n",
    "\n",
    "# neural network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    \n",
    "    # initialise the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrices, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc \n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "\t\t\n",
    "# train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # convert targets list to 2d array\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # output layer error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9: Train on the MNIST data with 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 784\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)\n",
    "\n",
    "# epochs is the number of times the training data set is used for training\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-3bb5a5212c3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# all_values[0] is the target label for this record\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m___\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# train the neural network\n",
    "\n",
    "for e in range(epochs):\n",
    "    # go through all records in the training data set\n",
    "    for record in training_data_list:\n",
    "        # split the record by the ',' commas\n",
    "        all_values = record.split(',')\n",
    "        # scale and shift the inputs\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "        # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = __\n",
    "        # all_values[0] is the target label for this record\n",
    "        targets[___] = 0.99\n",
    "        n.train(inputs, targets)\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/BYONN_Task9.py\n",
    "#Task 9\n",
    "epochs = 1\n",
    "# train the neural network\n",
    "\n",
    "for e in range(epochs):\n",
    "    # go through all records in the training data set\n",
    "    for record in training_data_list:\n",
    "        # split the record by the ',' commas\n",
    "        all_values = record.split(',')\n",
    "        # scale and shift the inputs\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "        # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "        # all_values[0] is the target label for this record\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        n.train(inputs, targets)\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the neural network\n",
    "\n",
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    # query the network\n",
    "    outputs = n.query(inputs)\n",
    "    # the index of the highest value corresponds to the label\n",
    "    label = numpy.argmax(outputs)\n",
    "    # append correct or incorrect to list\n",
    "    if (label == correct_label):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.5\n"
     ]
    }
   ],
   "source": [
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = numpy.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth comparing this score of just under 95% accuracy against industry benchmarks recorded at http://yann.lecun.com/exdb/mnist/. We can see that we’re better than some of the historic benchmarks, we are about the same performance as the simplest neural network approach listen there, which has a performance of 95.3%.\n",
    "That’s not bad at all. We should be very pleased that our very first go at a simple neural network achieves the kind of performance that a professional neural network researcher achieved.\n",
    "\n",
    "#### Some Improvements: Tweaking the Learning Rate\n",
    "A 95% performance score on the MNIST dataset with our first neural neural network, using only simple ideas and simple Python is not a bad at all, and if you wanted to stop here you would be entirely justified.\n",
    "\n",
    "Reproduce the following insights:\n",
    "\n",
    "*Let’s try doubling it to 0.6, to see if a boost will actually be helpful or harmful to the overall network learning. If we run the code we get a performance score of 0.9047. That’s worse than before. So it looks like the larger learning rate leads to some bouncing around and overshooting during the gradient descent.\n",
    "Let’s try again with a learning rate of 0.1. This time the performance is an improvement at 0.9523.*\n",
    "\n",
    "<img src=\"Figures/MYONN_Perf_LearningRate.png\" width = 300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Improvements: Doing Multiple Runs\n",
    "The next improvement we can do is to repeat the training several times against the data set. Some people call each run through an epoch. So a training session with 10 epochs means running through the entire training data set 10 times.\n",
    "\n",
    "<img src=\"Figures/MYONN_Perf_LearningRate_Epochs.png\" width = 300>\n",
    "\n",
    "You can see that calming down the learning rate did indeed produce better performance with more epochs. That peak of 0.9689 represents an approximate error rate of 3%, which is comparable to the networks benchmarks on Yann LeCun’s website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing Network Capacity\n",
    "\n",
    "<img src=\"Figures/MYONN_Perf_HiddenNodes.png\" width = 300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Training Data: Rotations\n",
    "\n",
    "If you think about the MNIST training data you realise that it is quite a rich set of examples of how people write numbers. There are all sorts of styles of handwriting in there, good and bad too.\n",
    "The neural network has to learn as many of these variations as possible. It does help that there are many forms of the number “4” in there. Some are squished, some are wide, some are rotated, some have an open top and some are closed.\n",
    "\n",
    "Wouldn’t it be useful if we could create yet more such variations as examples? How would we do that? We can’t easy collect thousands more examples of human handwriting. We could but it would be very laborious.\n",
    "A cool idea is to take the existing examples, and create new ones from those by rotating them clockwise and anticlockwise, by 10 degrees for example. For each training example we could have two additional examples. We could create many more examples with different rotation angles, but for now let’s just try $+10$ and $-10$ degrees to see if the idea works.\n",
    "\n",
    "Python’s many extensions and libraries come to the rescue again. The *ndimage.interpolation.rotate()* can rotate an array by a given angle, which is exactly what we need. Remember that our inputs are a one-dimensional long list of length 784, because we’ve designed our neural networks to take a long list of input signals. We’ll need to reshape that long list into a $28*28$ array so we can rotate it, and then unroll the result back into a $784$ long list of input signals before we feed it to our neural network.\n",
    "The following code shows how we use the *ndimage.interpolation.rotate()* function, assuming we have the *scaled_input* array from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.ndimage for rotating image arrays\n",
    "import scipy.ndimage\n",
    "\n",
    "# create rotated variations\n",
    "# rotated anticlockwise by 10 degrees\n",
    "inputs_plus10_img = scipy.ndimage.interpolation.rotate(scaled_input.reshape(28,28), 10, cval=0.01, reshape=False)\n",
    "# rotated clockwise by 10 degrees\n",
    "inputs_minus10_img = scipy.ndimage.interpolation.rotate(scaled_input.reshape(28,28), -10, cval=0.01, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "<img src=\"Figures/MYONN_RotateDigits.png\" width = 300>\n",
    "You can see the benefits clearly. The version of the original image rotated +10 degrees provides an example where someone might have a style of writing that slopes their 1’s backwards. Even more interesting is the version of the original rotated -10 degrees, which is clockwise. You can see that this version is actually straighter than the original, and in some sense a more representative image to learn from.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/BYONN_Task10.py\n",
    "# Task 10: Change the code from above that loops over the epochs to  \n",
    "#          include rotated versions of the digits\n",
    "\n",
    "# neural network class definition\n",
    "for e in range(epochs):\n",
    "    # go through all records in the training data set\n",
    "    for record in training_data_list:\n",
    "        # split the record by the ',' commas\n",
    "        all_values = record.split(',')\n",
    "        # scale and shift the inputs\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "        # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "        # all_values[0] is the target label for this record\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        n.train(inputs, targets)\n",
    "        \n",
    "        ## create rotated variations\n",
    "        # rotated anticlockwise by x degrees\n",
    "\t\tx=10\n",
    "        inputs_plusx_img = scipy.ndimage.interpolation.rotate(inputs.reshape(28,28), x, cval=0.01, order=1, reshape=False)\n",
    "        n.train(inputs_plusx_img.reshape(784), targets)\n",
    "        # rotated clockwise by x degrees\n",
    "        inputs_minusx_img = scipy.ndimage.interpolation.rotate(inputs.reshape(28,28), -x, cval=0.01, order=1, reshape=False)\n",
    "        n.train(inputs_minusx_img.reshape(784), targets)\n",
    "        \n",
    "        # rotated anticlockwise by 10 degrees\n",
    "        #inputs_plus10_img = scipy.ndimage.interpolation.rotate(inputs.reshape(28,28), 10, cval=0.01, order=1, reshape=False)\n",
    "        #n.train(inputs_plus10_img.reshape(784), targets)\n",
    "        # rotated clockwise by 10 degrees\n",
    "        #inputs_minus10_img = scipy.ndimage.interpolation.rotate(inputs.reshape(28,28), -10, cval=0.01, order=1, reshape=False)\n",
    "        #n.train(inputs_minus10_img.reshape(784), targets)\n",
    "        \n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/MYONN_Perf_RotateDigits.png\" width = 300>\n",
    "\n",
    "You can see that with 5 epochs the best result is $0.9745$ or 97.5% accuracy. That is a jump up again on our previous record.\n",
    "It is worth noticing that for large angles the performance degrades. That makes sense, as large angles means we create images which don’t actually represent the numbers at all. Imagine a “3” on its side at 90 degrees. That’s not a three anymore. So by adding training examples with overly rotated images, we’re reducing the quality of the training by adding false examples. Ten degrees seems to be the optimal angle for maximising the value of additional data.\n",
    "The performance for 10 epochs peaks at a record breaking 0.9787 or almost 98%! That is really a stunning result, amongst the best for this kind of simple network. Remember we haven’t done any fancy tricks to the network or data that some people will do,we’ve kept it simple, and still achieved a result to be very proud of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Comments\n",
    "\n",
    "- https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
